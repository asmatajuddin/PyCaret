{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "PyCaret Tutorial"
      ],
      "metadata": {
        "id": "3TqMBxlNZXek"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is PyCaret\n",
        "\n",
        "PyCaret is an open-source, low-code machine learning library in Python that automates machine learning workflows. It is an end-to-end machine learning and model management tool that exponentially speeds up the experiment cycle and makes you more productive.\n",
        "\n",
        "Compared with the other open-source machine learning libraries, PyCaret is an alternate low-code library that can replace hundreds of lines of code with a few lines only. This makes experiments exponentially faster and more efficient. PyCaret is essentially a Python wrapper around several machine learning libraries and frameworks such as scikit-learn, XGBoost, LightGBM, CatBoost, spaCy, Optuna, Hyperopt, Ray, and a few more.\n",
        "\n",
        "The design and simplicity of PyCaret are inspired by the emerging role of citizen data scientists, a term first used by Gartner. Citizen data scientists are power users who can perform both simple and moderately sophisticated analytical tasks that would previously have required more technical expertise.\n",
        "\n",
        "PyCaret is simple and easy to use. All the operations performed in PyCaret are sequentially stored in a pipeline that is fully orchestrated for deployment. Whether it’s imputing missing values, transforming categorical data, feature engineering, or even hyperparameter tuning, PyCaret automates all of it. To learn more about PyCaret, watch this one-minute video.\n",
        "\n",
        "Modules in PyCaret\n",
        "\n",
        "PyCaret’s API is arranged in modules. Each module supports a type of supervised learning (classification and regression) or unsupervised learning (clustering, anomaly detection, nlp, association rules mining). A new module for time series forecasting was released recently under beta as a separate pip package."
      ],
      "metadata": {
        "id": "pkO2VBACZYAa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installing PyCaret\n",
        "\n",
        "Installing PyCaret is simple through pip and it takes only a few minutes. PyCaret's default installation only installs hard dependencies as listed in the requirements.txt file on the repo.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jv00psf2ZdsV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pycaret\n"
      ],
      "metadata": {
        "id": "ufQ4NKHKZenR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pycaret[full]\n"
      ],
      "metadata": {
        "id": "bxTJ2feuZfwQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Features\n",
        "\n",
        "PyCaret’s claim to fame is its simplicity. Compared to other automated machine learning softwares, PyCaret is extremely flexible, comes with a unified API, and has no learning curve.\n",
        "\n",
        "PyCaret is loaded with functionalities. You can go from processing your data to training models, and then deploying them on the cloud within a few lines of code. It comes with a lot of preprocessing transformations that are applied automatically when the experiment is initialized. The model zoo of PyCaret has over 70 untrained models for supervised and unsupervised tasks."
      ],
      "metadata": {
        "id": "_T70vqphZiBH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also use PyCaret on GPU and speed up your workflow by 10x. To train models on GPU simply pass use_gpu = True in the setup function. No change in code at all.\n",
        "\n",
        "PyCaret is a glass-box solution. It comes with tons of functionality to interact with the model and analyze the performance and the results. All the standard plots like confusion matrix, AUC, residuals, and feature importance are available for all models. It also has integration with the SHAP library which is used to explain the output of any complex tree-based machine learning models."
      ],
      "metadata": {
        "id": "q1dc_C5bZlcn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PyCaret also integrates with MLflow for its MLOps functionalities. It automatically logs metrics, parameters, and artifacts when you pass log_experiment = True in the setup function."
      ],
      "metadata": {
        "id": "xGioG4pXZrnh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing in PyCaret\n",
        "\n",
        "All the preprocessing transformations are applied within the setup function. PyCaret provides over 25 different preprocessing transformations that are defined in the setup function."
      ],
      "metadata": {
        "id": "dIPGnrRxZsLS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Modules in PyCaret\n",
        "\n",
        "Module\tTask\n",
        "pycaret.classification\tSupervised: Binary or multi-class classification\n",
        "pycaret.regression\tSupervised: Regression\n",
        "pycaret.clustering\tUnsupervised: Clustering\n",
        "pycaret.anomaly\tUnsupervised: Anomaly Detection\n",
        "pycaret.nlp\tUnsupervised: Natural Language Processing (Topic Modeling)\n",
        "pycaret.arules\tUnsupervised: Association Rules Mining\n",
        "pycaret.datasets\tDatasets\n",
        "Example: An end-to-end ML use-case\n",
        "\n",
        "This is a typical machine learning workflow. Framing an ML problem correctly is the first step in any machine learning project and it may take from a few days to weeks in framing it right.\n",
        "\n",
        "Data Sourcing and ETL (Extract-Transform-Load) may also vary in complexity depending on the organization’s maturity in technology and size. It is normal to see a team of data engineers working together in this phase.\n",
        "\n",
        "The Exploratory Data Analysis (EDA) phase encompasses exploring the raw data to assess the quality of data (missing values, outliers, etc.), correlation among features, and test business hypotheses.\n",
        "\n",
        "For data preparation and model training and selection, PyCaret does all the heavy lifting under the hood. Data preparation includes transformations such as missing value imputation, scaling (min-max, z-score, etc.), categorical encoding (one-hot-encoding, ordinal encoding, etc.) feature engineering (polynomial, feature interaction, ratios, etc.), and feature selection.\n",
        "\n",
        "After data preparation, the model training and selection phase involves fitting multiple algorithms and evaluating performance using some kind of testing strategy (mostly cross-validation).\n",
        "\n",
        "Finally, the chosen (best model) is deployed as an API end-point for inference. Once deployment is done, monitoring API and keeping a tab on model performance in production goes on for life."
      ],
      "metadata": {
        "id": "jntSTSctZuc9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load the dataset from pycaret\n",
        "from pycaret.datasets import get_data\n",
        "data = get_data('diamond')\n"
      ],
      "metadata": {
        "id": "q3izzsvvZzSk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot scatter carat_weight and Price\n",
        "import plotly.express as px\n",
        "fig = px.scatter(x=data['Carat Weight'], y=data['Price'], facet_col = data['Cut'], opacity = 0.25, trendline='ols', trendline_color_override = 'red')\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "GPgv7Y4uZ00Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize setup\n",
        "from pycaret.regression import *\n",
        "s = setup(data, target = 'Price', transform_target = True, log_experiment = True, experiment_name = 'diamond')\n"
      ],
      "metadata": {
        "id": "jMBfBk5hZ2Lo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compare all models\n",
        "best = compare_models()\n"
      ],
      "metadata": {
        "id": "nq56wRY3Z4iy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check the final params of best model\n",
        "best.get_params()\n"
      ],
      "metadata": {
        "id": "QhnkWoSEZ7_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check the residuals of trained model\n",
        "plot_model(best, plot = 'residuals_interactive')\n"
      ],
      "metadata": {
        "id": "J_3QAmmuZ9da"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_model(best)\n"
      ],
      "metadata": {
        "id": "Zh2PEd98Z-0e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# within notebook\n",
        "!mlflow ui\n",
        "\n",
        "# from the command line\n",
        "mlflow ui\n"
      ],
      "metadata": {
        "id": "rgQma5r-aA_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pycaret-ts-alpha\n"
      ],
      "metadata": {
        "id": "-NDhUUQSaDgp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Time series data is one of the most common data types and understanding how to work with it is a critical data science skill if you want to make predictions and report on trends. If you would like to learn more about time-series analysis and forecasting in Python, DataCamp has a great collection of courses. You can start with Time Series with Python Learning track or one of these courses would also be a great choice:\n",
        "\n",
        "Introduction to Time Series Analysis in Python\n",
        "Machine Learning for Time Series Data in Python\n",
        "Time Series Analysis Tutorial\n",
        "PyCaret Integrations\n",
        "\n",
        "Area\tIntegrations\n",
        "Models\tscikit-learn, XGBoost, LightGBM, CatBoost\n",
        "GPU Training\tRAPIDS.AI\n",
        "Distributed Computing\tRAY\n",
        "Hyperparameter Tuning\ttune-sklearn, optuna\n",
        "Plotting & Analysis\tplotly, yellowbrick, seaborn, matplotlib\n",
        "NLP\tgensim, spacy, nltk\n",
        "MLOps\tMLflow\n"
      ],
      "metadata": {
        "id": "E7TOm957aIxc"
      }
    }
  ]
}